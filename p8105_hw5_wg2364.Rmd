---
title: "Homework 5"
author: "Wenhao Gou"
date: "2020/11/12"
output: github_document
---

```{r setup, message = F, warning = F}
knitr::opts_chunk$set(
  message = F,
  warning = F
)
library(tidyverse)
```

# Question 1:

## Include and tidy the data:

```{r Q1 include data}
homocide <- read_csv("Dataset/homicide_data/homicide-data.csv")
homocide
```

The dataset have 52,179 rows and 12 columns. There are 10 variables in the dataset:

* `uid`: a character indicate the id of the case

* `reported_date`: a number indicate the date of the case.

* `victim_last` to `victim_sex`: indicate the information of the victim (first and last name, race, age and sex). All of these variables are character. 

* `city` and `state`: two character indicate the location of the case

* `lat` and `lon`: two number indicate the latitude and longitude of the location

* `disposition` : a character indicate the disposition of the case

Next, we want to create a `city_state` variable (e.g. “Baltimore, MD”) and then summarize
within cities to obtain the total number of homicides and the number of unsolved homicides
(those for which the disposition is “Closed without arrest” or “Open/No arrest”)

```{r}
homocide <- 
  homocide %>% 
  mutate(city_state = str_c(city, state, sep = "_")) %>% 
  mutate(result = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )) 
homocide_summary <-
  homocide %>% 
  select(city_state, result) %>% 
  group_by(city_state) %>% 
  summarise(
    total_case = n(),
    total_unsolved = sum(result == "unsolved")
  )
homocide_summary
```

## Proportion test on Baltimore_MD

We can firstly define a function to do all the jobs:

```{r}
prop_test_ <- function(tibble_input){
  tibble_output <-
    tibble_input %>% 
    mutate(proptest = 
              map2(
                .x = pull(tibble_input, total_unsolved),
                .y = pull(tibble_input, total_case),
                ~prop.test(x = .x, n = .y))) %>% 
    mutate(tidytest = 
             map(
               .x = proptest, 
               ~broom::tidy(.x))) %>% 
    select(-proptest) %>% 
    unnest(tidytest) %>% 
    select(city_state, estimate, conf.low, conf.high)
  return(tibble_output)
}
```

Then, for Baltimore, MD
```{r}
homocide_summary %>%
  filter(city_state == "Baltimore_MD") %>%
  prop_test_(.) 
```

## Run the test for all the city:
```{r}
homocide_result <-
  homocide_summary %>% 
  prop_test_(.)
homocide_result
```

## The plot:
```{r}
homocide_result %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  filter(city_state != "Tulsa_AL") %>%   #Poor data with just 1 case
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


# Question 2:

## Read and tidy the data:

We can read the data by maps:

```{r}
study_data <- 
  tibble(
    names = list.files("Dataset/lda_data")) %>% 
  mutate(file_names = str_c("Dataset/lda_data/" , names)) %>% 
  mutate(content = map(.x = file_names, ~read_csv(.x))) %>% 
  select(-file_names)
study_data
```


Next, we need to tidy the dataset:

```{r}
study_data_tidy <-
  study_data %>%
  unnest(content) %>% 
  separate(names,
           into = c("arm","subject_id"),
           sep = "_") %>% 
  mutate(arm = case_when(
    arm == "con" ~ "Control",
    arm == "exp" ~ "Expose"
  )) %>% 
  mutate(subject_id = str_extract(subject_id, "^\\d{2}")) %>% 
  mutate(subject_id = as.numeric(subject_id)) %>% 
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observation",
               names_prefix = "week_") %>% 
  mutate(week = as.numeric(week))
study_data_tidy
```

## Spaghetti plot 

```{r}
study_data_tidy %>%
  mutate(subject_id = as.factor(subject_id)) %>% 
  ggplot(aes(x = week, y = observation, color = subject_id)) +
  geom_point() +
  geom_line() +
  facet_grid(~arm) +
  ggtitle("Spaghetti plot of different group")
```

From the plot, we can see that, there are different pattern between the control group and the expose group. For the control group, the observation tend to be fluctuate around 1.25 for 8 weeks. For the expose group, there is an increasing trend of the observation over time. 

# Question 3:

## The main function for the simulation:

```{r}
set.seed(429)

simulation_ <- function(mu, sd = 5, sample_size = 30, iteration = 5000){
  df <- 
    tibble(
    sample_id = 1:iteration,
    sample_data = rerun(iteration, rnorm(n = sample_size, mean = mu, sd = sd)))
  df <- 
    df %>% 
    mutate(ttest = map(.x = sample_data, ~t.test(.x, mu = 0))) %>% 
    mutate(ttidy = map(.x = ttest, ~broom::tidy(.x))) %>% 
    select(sample_id, ttidy) %>%
    unnest(ttidy) %>% 
    select(sample_id, estimate, p.value) %>% 
    mutate(decision = case_when(
      p.value < 0.05 ~ "Reject",
      p.value >= 0.05 ~ "Fail to Reject"
    ))
  return(df)
}
```

## Simulation on mean equal to 0:
```{r}
simulation_(0)
```

## Simulation on multiple means:

```{r}
simulation_prop_ <- function(simulation_result){
  output <- 
    simulation_result %>% 
    pull(decision)
  return(sum(output== "Reject")/nrow(simulation_result))
}

Data <- tibble(
  mu = 1:6
)

Data <-
  Data %>% 
  mutate(raw_data = map(.x = mu, ~simulation_(.x))) %>% 
  mutate(proportion = map(.x = raw_data, ~simulation_prop_(.x))) %>% 
  unnest(proportion)
```


## Plots:

### Plot A: Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of on the x axis. 


```{r}
Data %>% 
  select(-raw_data) %>% 
  ggplot(aes(x = mu, y = proportion)) +
  geom_line() +
  geom_point() +
  ggtitle("Relationship between effect size and power")

```

Comments:

We can see that as the _mu_ increasing, the proportion of rejection will increasing too. This is because, according to the central limit theorem (CLT), for a fixed sample size (n = 30 in this case), the distribution of the sample mean will follow a normal distribution with _mu_ = mean we set and _sd_ = _sigma/sqrt(n)_

So, under a fixed significance level, proportion of times the null was rejected will increase as the peak of the (normal) distribution of sample mean will moving leftward. This will give a  bigger intersection are with the rejection area of the null hypothesis.  

### Plot B: Make a plot showing the average estimate of _mu_ on the y axis and the true value of _mu_ on the x axis. Make a second plot (or overlay on the first) the average estimate of _mu_ only in samples for which the null was rejected on the y axis and the true value of on the x axis

```{r}
total_mean_estimation <-
  Data %>% 
  select(-proportion) %>% 
  unnest(raw_data) %>% 
  group_by(mu) %>% 
  summarise(total_mean_est = mean(estimate)) %>% 
  as.tibble()

reject_mean_estimation <-
  Data %>% 
  select(-proportion) %>% 
  unnest(raw_data) %>% 
  group_by(mu, decision) %>% 
  summarise(reject_mean_est = mean(estimate)) %>% 
  filter(decision == "Reject") %>% 
  select(-decision) %>% 
  as_tibble()
  

left_join(total_mean_estimation, reject_mean_estimation) %>% 
  pivot_longer(total_mean_est:reject_mean_est,
               names_to = "group",
               values_to = "estimation") %>% 
  ggplot(aes(x = mu, y = estimation, group = group)) +
  geom_point(aes(color = group)) +
  geom_line(aes(color = group)) +
  ggtitle("Simulation difference among two groups")
```

Comments:

In general, we can see that as the _mu_ increase, the divergent of these two sets will be narrower. Also, we notice that the total mean is approximately equal to the true _mu_. Again, this is result from CLT and as the _mu_ increase, the distribution of sample mean will follows a normal distribution with increasing _mu_ and the proportion of rejection (H_0: _mu_ = 0) will get bigger. 